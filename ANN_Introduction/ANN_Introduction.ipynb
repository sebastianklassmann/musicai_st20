{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A very short Introduction to Artificial Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![NeuralNet](https://cdn.pixabay.com/photo/2018/08/28/13/20/neural-network-3637503_1280.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Author:** Sebastian Klassmann, sklassm1@uni-koeln.de  \n",
    "\n",
    "Date: May 26$^{th}$, 2020  \n",
    "Libraries used: nnone, this notebook is presentation only.  \n",
    "Python Version: 3.7.7 \n",
    "Other dependencies: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Please be aware that this notebook is merely a short introduction to a few terms and concepts that are neccessary for a practical understanding of basic artificial neural networks. \n",
    "In fact, it is quite impossible to cover all the ground we are trying to cover today in just over an hour. We will still try, albeit at the expense of conceptual depth.\n",
    "\n",
    "For a more in-depth introduction and additional depth of discussion, **please refer to the references given at the end of this notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. What is the core concept of Artificial Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Artificial Neural Networks are at the very core of research strategies that have been subsumed as *machine learning* or *deep learning* approaches to data-driven science over the past decades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To quote from Anderson 2014, ANNs are \"mathematical methods for input:output mappings. In practice they usually share some subset of the following features:\n",
    "1. They are inspired by biology, but are not expected to slavishly emulate biology.\n",
    "2. They are implemented as computer programs.\n",
    "3. They have 'nodes' that play the role of neurons, collections of neurons, brain structures or cognitive modules. Their role depends on the modeller's intent and the problem being modelled.\n",
    "4. Nodes are interconnected. [...]\n",
    "5. The individual nodes produce outputs from inputs. The network's output is the collection of all these local operations\" (p. 82)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, that does not tell us too much just yet, does it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore, what does that mean for us? Let us consider the following basic schematization of an Artificial Neural Network (ANN):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='ann1.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What would this simple architecture do in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Given a suitable input signal to the input layer, the model would transform the input to a potentially very different output at the output layer. This transformation is solely based on the connection between the nodes, as well as certain predispositions of the nodes at hand.\n",
    "* All partial operations in the network can be expressed using very simple mathematical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's see this in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A very much simplified ANN:\n",
    "<img src='ann2.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As you can see, the above schematization has been expanded by adding gravity to the connections from given nodes to other nodes. These are called 'weights' (w). For simplicity, they have not been given index numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let us assume a simple input to this neural net, let's make it \n",
    "$\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "\\end{pmatrix} \n",
    "$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='ann3.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This input (in this simplified case: *activation at the input layer*) is then passed on along the arrows (edges) of the network. The weights of the connections directly influence the activation input to the next layer of nodes. The activation in any of the subsequent nodes will be equal to the sum of all incoming weighted activations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='ann4.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The very same holds true for the resulting activation passed on by the output layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='ann5.png' width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As you can see, the output of our simplified network given the input $\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "\\end{pmatrix} \n",
    "$ would be a single float (1.4), which looks a little different than the input we passed to the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wait, what? You said \"very much simplified\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes, the previous schematics have been a little simpler than what is usually used for building ANNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In fact, we need a few extra assumptions that are usually made:  \n",
    "* Each node / neuron in our network will usually be considered to have a certain tendency to show activation based on an input. This is expressed as a *bias* parameter (b). These biases are the other (the first one being the weights) set of parameters that can drastically change the performance of our network structure.\n",
    "* Usually, we want to limit the potential values weighted activations can show by means of a nonlinear function that *squashes* the activation values. One very common example is the so-called **sigmoid activation function** for a weighted activation y after application of the given non-linearity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "y = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "<img src='sigmoid_wiki.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "While x is the sum of weighted activations being passed to the node at hand, i.e.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "y = \\frac{1}{1+e^{-(\\sum_{x}i_x w_x + b_y)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or, a little simpler:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "Y = \\frac{1}{1+e^{-(I \\cdot W + B)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\rightarrow$ Yes, those capital letters hint at matrices.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\rightarrow$ Yes, they make your life easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\rightarrow$ We will get to this in a second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's not go too deep on this for now, it is actually very simple, once you can use a machine do the computations for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://cdn.pixabay.com/photo/2017/07/10/23/43/question-mark-2492009__340.jpg' width=1300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perspectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At this stage, you might be wondering: How can this all be applied in a productive fashion? And how do we get from this to something like the LSTM-results we were shown in the beginning of this session?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let us conduct a little thought experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have seen that an ANN takes a compatible input signal, passes on activations based on it's inherent connections and thereby transforms it to a given output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![cr: O'Reilly Media](https://www.oreilly.com/library/view/practical-convolutional-neural/9781788392303/assets/246151fb-7893-448d-b9bb-7a87b387a24b.png)\n",
    "\n",
    "Imagine an ANN with some randomly chosen weighted connection between nodes and some randomly chosen biases for the different nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://cdn.pixabay.com/photo/2015/03/07/03/46/white-shepherd-662744__340.jpg' width=800>\n",
    "\n",
    "\n",
    "Imagine a picture of a dog, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Computers are basically stupid. Therefore, a computer will not be able to tell you whether or not it is actually looking at a dog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imagine, that we were able to preprocess the image in a way that converts the image to an array of numbers (think of pixel-wise greyscale values, for exampe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![glueckskeks Pixabay](https://cdn.pixabay.com/photo/2017/07/14/08/23/fortune-cookies-2503077__480.jpg)\n",
    "\n",
    "Further, Imagine, that we have stored the information that the animal shown has the property of being a dog written on a virtual piece of paper (a label, if you will) and stored away safely, so that our ANN cannot see it while processing the picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imagine that this information is encoded in a format that is structurally very much compatible to the output generated by our network. The simplest case (that is probably technically useless) would be a single float."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imagine that we allow our network to compare the output to the label. Be assured that our network's judgement will probably be very bad, as all the parameters making up the connections in our network were chosen randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now imagine that we have a thousand images of varying mammals, a certain amount of them being dogs.  \n",
    "<img src='https://cdn.pixabay.com/photo/2016/08/12/13/01/collage-1588416__340.jpg' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\rightarrow$ Ok, fine. These are all dogs. But then again, your computer wouldn't know. Would it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imagine that we pass all of them (including non-dogs) through our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imagine that we make our network just a little smarter by implementing a mathematical way of calculating the error of the outputs compared to the labels of the respective pictures. If we were able to find an algorithm for slightly adjusting those values of the network that we can influence (i.e.: the weights and biases, as we cannot alter the inputs from the training set) in order to make it slightly better at differentiating dogs from non-dogs in our training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ0yUENFTNbKRDg-cTH0h2WYjejTQRhggD5Mz7SsEev9MxSFtR_gA' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The above equations are the core of an algorithm called *backpropagation*.  \n",
    "  \n",
    "In a network that connects many layers of nodes with each other, adjusting a single weight from the first to the second layer will change the weighted activation that is passed on through the entirety of the remaining network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore, **it is not possible to start modifying our parameters from the first layer** on. It is much more productive to calculate the error at the output layer and then **look back through the network in order to gently nudge the parameters in a direction that minimizes the error, working (propagating) our way backwards to the whole network**. This algorithm will have to arrive at the point of how we can best adjust the parameters related to the first layer, while keeping in mind optimal changes to all successive layers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\rightarrow$ This may seem daunting and is indeed a little difficult to calculate by hand. Luckily, most machine learning libraries (inlcuding Tensorflow / Keras) will just be able to quickly do the required calculations for you. Therefore, in order to work on machine / deep learning models, you do not need to be able to calculate the backpropagation of a loss value by hand. It is, however, beneficial to have a basic understanding of how this procedure works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Imagine that we save the slightly improved values for our parameters and confront the modified network with the same set of pictures, over and over again. Every time (iteration, or: epoch), it will get slightly better at performing the classification task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\rightarrow$ This is the core concept of (supervised) Machine Learning (using feed-forward networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You want more dog examples? Please feel free to consult Skansi (2018), pp. 50-78."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our next session, we will be looking at a classification problem that has been called the \"hello world\" algorithm of Machine Learning: The automatized recognition of handwritten digits from a well-known corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\rightarrow$ I will be sending out additional reading material to supplement today's *tour-de-force*. Please make sure that you read them (please expect the texts by Thursday, this week)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\rightarrow$ The assignment about todays session will focus on concepts that we dealt with today, as well as a little bit of vector and matrix calculations in Python (Numpy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\rightarrow$ There will be an additional / alternative assignment based on one of the texts I will be sending out this week. This one is a basic case for a classification problem and does not require any knowledge in Tensorflow. You can solve it using Numpy alone. Please remember that your choice in assignments is entirely up to you. This one might be a challenge, but it's well worth a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References:  \n",
    "Anderson, B. (2014). *Computational neuroscience and cognitive modelling: a student's introduction to methods and procedures*. Sage.  \n",
    "Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep learning*. MIT press.  \n",
    "Rashid, T. (2017). *Neuronale Netze selbst programmieren: ein verst√§ndlicher Einstieg mit Python*. O'Reilly.  \n",
    "Skansi, S. (2018). *Introduction to Deep Learning: From Logical Calculus to Artificial Intelligence*. Springer.  \n",
    "  \n",
    "----\n",
    "\n",
    "**Additionally, spend some of your free time on this excellent [video series](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)**."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.7.7",
   "language": "python",
   "name": "py3pack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
